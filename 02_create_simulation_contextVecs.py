# This file creates window-specific context vectors for the simulation data & saves the fitted LSTM autoencoders. 
import time 
startTime = time.time()

#################################################### USER SET-UP ####################################################
#Survival model ("aft" or "cox")
surv_model = "cox"

#Decision point time
t = 3

#Number of cores to parallelize across
num_cpu = 3

#Directory path of folder containing data generated by 01_generate_simulation_data.R
input_path = "/home/gmrhodes/Project2/simulations/data/"

#Directory path of folder to write KerasTuner data to
##Must contain sub-directories "aft" and "cox"
tuner_path = "/home/gmrhodes/Project2/simulations/tuner/"

#Directory path of output folder to write results to
##Must contain sub-directories "aft" and "cox"
output_path = "/home/gmrhodes/Project2/simulations/context_vectors/"


#################################################### SET-UP ####################################################
seed_number=5678
import os
os.environ['PYTHONHASHSEED']='0'
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import random
random.seed(seed_number)
import numpy as np
np.random.seed(seed_number)
import kerastuner as kt
import tensorflow as tf
from tensorflow.keras import layers as tfkl
tf.random.set_seed(seed_number)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
import pandas as pd
from sklearn import preprocessing
import multiprocessing as mp

#Padding value for missing time steps
padVal = -9999 

#LSTM autoencoder hyperparameters
epochs = 500 #training epochs
cv_dimension = 5 #context vector dimension


#################################################### FUNCTION TO SCALE COVARIATES ####################################################
def minMaxScale(df):
    #Select continuous longitudinal covariates
    contCovDF = df[["long_cov1", "long_cov2", "long_cov3"]].copy()

    #Min-max scale data
    scaler = preprocessing.MinMaxScaler()
    dfStand = scaler.fit_transform(contCovDF)
    dfStand = pd.DataFrame(dfStand, columns=contCovDF.columns)
    
    #Replace scaled variables in original dataset
    df[["long_cov1", "long_cov2", "long_cov3"]] = dfStand.values
    return df


#################################################### FUNCTION TO CREATE 3D COVARIATE ARRAY ####################################################
def covArray_construct(covDF):    
    #Create padded 3D numpy arrays of covariates for Keras
    gb = covDF.groupby(['id'])
    maxBlocks = gb['id'].size().max() 
    covArr = np.array([np.pad(frame['time'].values, pad_width=(0, maxBlocks-len(frame)), mode='constant', constant_values=padVal) for _,frame in gb]).reshape(-1, maxBlocks, 1)
    for col in covDF.columns[2:]:
        newArr = np.array([np.pad(frame[col].values, pad_width=(0, maxBlocks-len(frame)), mode='constant', constant_values=padVal) for _,frame in gb]).reshape(-1, maxBlocks, 1)    
        covArr = np.dstack((covArr,newArr))
    return covArr


#################################################### FUNCTIONS TO DEFINE LSTM AUTOENCODER #################################################### 
#Class returns the output of an LSTM layer stacked on a RepeatVector layer with the mask propogated through both layers
class lstm_bottleneck(tf.keras.layers.Layer):
    def __init__(self, lstm_units, time_steps, **kwargs):
        self.lstm_units = lstm_units
        self.time_steps = time_steps
        self.lstm_layer = tfkl.LSTM(units=lstm_units, activation='relu', return_sequences=False)
        self.repeat_layer = tfkl.RepeatVector(time_steps)
        super(lstm_bottleneck, self).__init__(**kwargs)

    def call(self, inputs):
        return self.repeat_layer(self.lstm_layer(inputs))

    def compute_mask(self, inputs, mask=None):
        return mask

#Class creates HyperModel object for LSTM autoencoder
class lstmHyperModel(kt.HyperModel):
    def __init__(self, padVal, timeSteps):
        self.padVal = padVal
        self.timeSteps = timeSteps
        
    def build(self, hp):
        input_layer = tfkl.Input(shape=(self.timeSteps, 1))
        x = tfkl.Masking(mask_value=self.padVal)(input_layer)
        x = lstm_bottleneck(lstm_units=cv_dimension, time_steps=self.timeSteps)(x)
        x = tfkl.LSTM(units=cv_dimension, activation='relu', return_sequences=True)(x)
        x = tfkl.TimeDistributed(tfkl.Dense(1))(x)
        lstm_ae = tf.keras.models.Model(inputs=input_layer, outputs=x)
        lstm_ae.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3,1e-4])), loss='mse')
        return lstm_ae
    

#################################################### FUNCTION TO CONSTRUCT WINDOW-SPECIFIC CONTEXT VECTORS #################################################### 
#Function to construct window-specific context vectors for longitudinal covariate 'cov' at decision point time 't'
def context_vector_construct(longCov, longName, cov, t):
    random.seed(seed_number)
    np.random.seed(seed_number)
    tf.random.set_seed(seed_number)
    
    #Set-up tensorflow session
    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
    sess = tf.compat.v1.Session(config=session_conf)
    tf.compat.v1.keras.backend.set_session(sess)

    #Construct LSTM HyperModel & conduct cross-validation to select learning rate
    longCov_lstm = longCov[:,:,np.newaxis]
    lstm_hyperMod = lstmHyperModel(padVal=padVal, timeSteps=longCov_lstm.shape[1])
    tuner = kt.tuners.RandomSearch(lstm_hyperMod, objective='val_loss', max_trials=2, seed=8, overwrite=True, 
                                           directory='{}{}/'.format(tuner_path, surv_model), project_name='t{}_cov{}'.format(t,cov))
    tuner.search(longCov_lstm, longCov_lstm, epochs=epochs, validation_split=0.2, verbose=0)
    
    #Retrieve model with "best" learning rate & fit to data
    lstm_best = tuner.get_best_models(num_models=1)[0]
    lstm_best.fit(longCov_lstm, longCov_lstm, epochs=epochs, verbose=0)

    #Extract context vector from LSTM autoencoder & save fitted model
    contextVec_model = tf.keras.Model(inputs=lstm_best.inputs, outputs=lstm_best.layers[2].output)
    contextVec_model.save('{}{}/model_t{}_cov{}'.format(output_path, surv_model, t, cov))
    cv = contextVec_model.predict(longCov_lstm)[:,0,:]  
    
    #Save variable names 
    contVecNames = list()
    for j in np.arange(1, cv_dimension+1, 1):
        contVecNames.append('{}_{}'.format(longName,j))
    
    #Reset tensorflow session & seeds 
    del tuner
    del lstm_hyperMod
    del lstm_best
    del contextVec_model
    tf.compat.v1.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()    
    random.seed(seed_number)
    np.random.seed(seed_number)
    tf.random.set_seed(seed_number) 
    
    #Return window-specific context vector & associated variable names
    return (cv, contVecNames)      
        
    
#################################################### FUNCTION TO COLLECT PARALLELIZED RESULTS ####################################################
def get_lstm_results(result):
    #Save context vector
    global context_vector_list
    context_vector_list.append(result[0])
    #Save context vector name
    global contVec_name_list
    contVec_name_list.append(result[1])
    
    
#################################################### MAIN ####################################################
if __name__ == '__main__':    
    #Read in data       
    df = pd.read_csv('{}{}_df.csv'.format(input_path,surv_model)) 
    
    #Min-max scale longitudinal covariates
    df = minMaxScale(df)
    
    #Keep only measurements taken prior to/at t on patients at risk at t
    simDF_t = df[(df.time<=t) & (df.U_L>t)].copy()
    
    #Create response dataframe 
    respDF = simDF_t[["id", "U", "delta", "U_L", "delta_L", "base_cov"]].copy()
    respDF['restricted_residual_life'] = respDF['U_L']-t
    respDF = respDF.drop_duplicates(subset='id', keep='first')
    respDF.index = range(0,len(respDF))
    
    #Create 3D covariate array (necessary for Keras)
    covDF = simDF_t[["id", "time", "long_cov1", "long_cov2", "long_cov3"]].copy()
    covArr = covArray_construct(covDF)
    longArr = np.delete(covArr,[0],2)
    longNames = list(covDF.columns)[2:]
    
    #Create lists to save context vectors & corresponding variable names
    context_vector_list = list()    
    contVec_name_list = list()     
    
    #Create context vector for each longitudinal covariate
    pool = mp.Pool(num_cpu)
    asyncResults = [pool.apply_async(context_vector_construct, args=(longArr[:,:,i], longNames[i], i, t), callback=get_lstm_results) for i in np.arange(0,longArr.shape[2])]       
    pool.close()
    pool.join()   
            
    #Combine baseline covariates, context vectors, and response variables
    regressMatrix = pd.concat([respDF, pd.DataFrame(np.hstack(context_vector_list), columns=np.hstack(contVec_name_list))], axis=1)
        
    #Write dataframe
    regressMatrix.to_csv('{}{}/contextVec_t{}.csv'.format(output_path,surv_model,t), index=False)
    print ('The script took {0} seconds.'.format(time.time() - startTime))
    
    
    
        
        
    
    



   









